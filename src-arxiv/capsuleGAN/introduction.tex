\section{Introduction}
\label{sec:introduction}
Generative modeling of data is a challenging machine learning problem that has garnered tremendous interest recently, partly due to the invention of generative adversarial networks (GANs)~\cite{bib:gan} and its several sophisticated variants~\footnote{https://github.com/hindupuravinash/the-gan-zoo}. A GAN model is typically composed of two neural networks; (1) a generator that attempts to transform samples drawn from a  prior distribution to samples from a complex data distribution with much higher dimensionality, and (2) a discriminator that decides whether the given sample is real or from the generator's distribution. The two components are trained by playing an adversarial game. GANs have shown great promise in modeling highly complex distributions underlying real world data, especially images. However, they are notorious for being difficult to train and have problems with stability, vanishing gradients, mode collapse and inadequate mode coverage~\cite{bib:dcgan, bib:igan, bib:gman}. Consequently, there has been a large amount of work towards improving GANs by using better objective functions~\cite{bib:wgan, bib:iwgan, bib:began}, sophisticated training strategies~\cite{bib:igan}, using structural hyperparameters~\cite{bib:dcgan, bib:acgan} and adopting empirically successful tricks~\footnote{https://github.com/soumith/ganhacks}.

Radford et al.~\cite{bib:dcgan} provide a set of architectural guidelines, formulating a class of convolutional neural networks (CNN) that have since been extensively used to create GANs (referred to as Deep Convolutional GANs or DCGANs) for modeling image data and other related applications~\cite{bib:im_synth, bib:im2im_tr}. More recently, however, Sabour et al.~\cite{bib:capsnet} introduced capsule networks (CapsNets) as a powerful alternative to CNNs, which learn a more \emph{equivariant} representation of images that is more robust to changes in pose and spatial relationships of parts of objects in images~\cite{bib:transauto} (information that CNNs lose during training, by design). Inspired by the working mechanism of optic neurons in the human visual system, capsules were first introduced by Hinton et al.~\cite{bib:transauto} as locally invariant groups of neurons that learn to recognize visual entities and output activation vectors that represent both the presence of those entities and their properties relevant to the visual task (such as object classification). CapsNets have been shown to outperform CNNs on MNIST digit classification and segmentation of overlapping digits~\cite{bib:capsnet}. This motivates the question whether GANs can be designed using CapsNets (instead of CNNs) to improve their performance.

We propose Generative Adversarial Capsule Network (CapsuleGAN) as a framework that incorporates capsules within the GAN framework. In particular, CapsNets are used as discriminators in our framework as opposed to the conventionally used CNNs. We show that CapsuleGANs perform better than CNN-based GANs at modeling the underlying distribution of MNIST~\cite{bib:mnist} and CIFAR-10~\cite{bib:cifar} datasets both qualitatively and quantitatively using the generative adversarial metric (GAM)~\cite{bib:gam} and at semi-supervised classification using unlabeled GAN-generated images with a small number of labeled real images.

The rest of the paper is organized as follows. Section~\ref{sec:related_work} discusses related work. In Section~\ref{sec:prelim} we provide a brief introduction to GANs and CapsNets. Section~\ref{sec:capsgan} describes our CapsuleGAN framework along with implementation guidelines. Qualitative and quantitative analyses of our model are presented in Section~\ref{sec:evaluation}. Section~\ref{sec:conclusion} concludes the paper and provides directions for future research.